{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet与ResNeXt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DenseNet的结构\n",
    "DenseNet的每一层都都与前面层相连，实现了特征重用。\n",
    "\n",
    "下图表示一个DenseBlock\n",
    "![DenseBlock](pics/denseblock.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图所示,在一个DenseBlock中,第i层的输入不仅与i-1层的输出相关,还有所有之前层的输出有关.记作:\n",
    "\n",
    "![output](pics/ttt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DenseNet网络的搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Growth_rate\n",
    "在一个DenseBlock里面，每个非线性变换H输出的channels数为恒定的Growth_rate，那么第i层的输入的channels数便是k+（i+1）* Growth_rate, k为Input的\n",
    "\n",
    "channels数，比如，假设我们把Growth_rate设为4，上图中H1的输入的size为8 * 32 * 32，输出为4 * 32 * 32， 则H2的输入的size为12 * 32 * 32，输出还是\n",
    "\n",
    "4 * 32 * 32，H3、H4以此类推，在实验中，用较小的Growth_rate就能实现较好的效果。\n",
    "\n",
    "#### Transition Layer\n",
    "请注意， 在一个DenseBlock里面，feature size并没有发生改变，因为需要对不同层的feature map进行concatenate操作，这需要保持相同的feature size。\n",
    "\n",
    "因此在相邻的DenseBlock中间使用Down Sampling来增大感受野，即使用Transition Layer来实现，一般的Transition Layer包含BN、Conv和Avg_pool，\n",
    "\n",
    "同时减少维度，压缩率(compress rate)通常为0.5， 即减少一半的维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DenseNet](pics/densenet.png)\n",
    "\n",
    "例如，假设block1的输出c * w * h是24 * 32 * 32，那么经过transition之后，block2的输入就是12 * 16 * 16。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottleneck\n",
    "为了减少参数和计算量，DenseNet的非线性变换H采用了Bottleneck结构      \n",
    "\n",
    "BN-ReLU-Conv(1×1)-BN-ReLU-Conv(3×3)，1×1的卷积用于降低维度，将channels数降\n",
    "\n",
    "低至4 * Growth_rate。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 网络搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. 引入需要的package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. 定义Transition Layer、Bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(nn.Module):\n",
    "    '''\n",
    "        transition layer is used for down sampling the feature\n",
    "        \n",
    "        when compress rate is 0.5, out_planes is a half of in_planes\n",
    "    '''\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv(F.relu(self.bn(x)))\n",
    "        # use average pooling change the size of feature map here\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    '''\n",
    "        the above mentioned bottleneck, including two conv layer, one's kernel size is 1×1, another's is 3×3\n",
    "\n",
    "        after non-linear operation, concatenate the input to the output\n",
    "    '''\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        \n",
    "        # input and output are concatenated here\n",
    "        out = torch.cat([out,x], 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 定义DenseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        '''\n",
    "        Args:\n",
    "            block: bottleneck\n",
    "            nblock: a list, the elements is number of bottleneck in each denseblock\n",
    "            growth_rate: channel size of bottleneck's output\n",
    "            reduction: \n",
    "        '''\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        num_planes = 2*growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
    "        \n",
    "        # a DenseBlock and a transition layer\n",
    "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
    "        num_planes += nblocks[0]*growth_rate\n",
    "        # the channel size is superposed, mutiply by reduction to cut it down here, the reduction is also known as compress rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans1 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "        \n",
    "        # a DenseBlock and a transition layer\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1]*growth_rate\n",
    "        # the channel size is superposed, mutiply by reduction to cut it down here, the reduction is also known as compress rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        # a DenseBlock and a transition layer\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2]*growth_rate\n",
    "        # the channel size is superposed, mutiply by reduction to cut it down here, the reduction is also known as compress rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        # only one DenseBlock \n",
    "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
    "        num_planes += nblocks[3]*growth_rate\n",
    "\n",
    "        # the last part is a linear layer as a classifier\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "        self.linear = nn.Linear(num_planes, num_classes)\n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        \n",
    "        # number of non-linear transformations in one DenseBlock depends on the parameter you set\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 根据参数定义所需的DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet(args):\n",
    "    '''args is a list for the number of layers'''\n",
    "    return DenseNet(Bottleneck, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 自定义dataset、划分训练集和验证集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. 自定义Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import PIL\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, labels, root, subset=False, transform=None):\n",
    "        self.labels = labels\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # iloc[i, c]: get the i_th data in c column\n",
    "        img_name = self.labels.iloc[idx, 0]\n",
    "        # get full path\n",
    "        full_name = os.path.join(self.root, img_name)\n",
    "        # open img as RGB\n",
    "        image = PIL.Image.open(full_name).convert('RGB')\n",
    "        \n",
    "        labels = self.labels.iloc[idx, 2]\n",
    "        \n",
    "        if self.transform != None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, int(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义目录、标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = './data/'\n",
    "\n",
    "classes = os.listdir(data_dir + 'train/')\n",
    "classes = sorted(classes, key=lambda item: (int(item.partition(' ')[0])\n",
    "                               if item[0].isdigit() else float('inf'), item))\n",
    "num_to_class = dict(zip(range(len(classes)), classes))\n",
    "num_to_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义训练数据的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = []\n",
    "for index, label in enumerate(classes):\n",
    "    path = data_dir + 'train/' + label + '/'\n",
    "    for file in os.listdir(path):\n",
    "        train.append(['{}/{}'.format(label, file), label, index])\n",
    "    \n",
    "df = pd.DataFrame(train, columns=['file', 'category', 'category_id',]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 随机分离训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.sample(frac=0.8)\n",
    "valid_data = df[~df['file'].isin(train_data['file'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. 定义transform、dataset、dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trans = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "valid_trans = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_set = MyDataset(train_data, data_dir + 'train/', transform = train_trans)\n",
    "valid_set = MyDataset(valid_data, data_dir + 'train/', transform = valid_trans)\n",
    "# test_set = MyDataset(sample_submission, data_dir + 'test/', transform = valid_trans)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)\n",
    "# test_loader  = DataLoader(test_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 记录数据集size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {\n",
    "    'train': len(train_loader.dataset), \n",
    "    'valid': len(valid_loader.dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 训练与测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. 作图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_curve(ys, title):\n",
    "    \"\"\"\n",
    "    plot curlve for Loss and Accuacy\n",
    "    Args:\n",
    "        ys: loss or acc list\n",
    "        title: loss or accuracy\n",
    "    \"\"\"\n",
    "    x = np.array(range(len(ys)))\n",
    "    y = np.array(ys)\n",
    "    plt.plot(x, y, c='b')\n",
    "    plt.axis()\n",
    "    plt.title('{} curve'.format(title))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('{}'.format(title))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def train(epoch, model, lossFunction, optimizer, device, trainloader):\n",
    "    \"\"\"train model using loss_fn and optimizer. When this function is called, model trains for one epoch.\n",
    "    Args:\n",
    "        train_loader: train data\n",
    "        model: prediction model\n",
    "        loss_fn: loss function to judge the distance between target and outputs\n",
    "        optimizer: optimize the loss function\n",
    "        get_grad: True, False\n",
    "    output:\n",
    "        total_loss: loss\n",
    "        average_grad2: average grad for hidden 2 in this epoch\n",
    "        average_grad3: average grad for hidden 3 in this epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    model.train()     # enter train mode\n",
    "    train_loss = 0    # accumulate every batch loss in a epoch\n",
    "    correct = 0       # count when model' prediction is correct i train set\n",
    "    total = 0         # total number of prediction in train set\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device) # load data to gpu device\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        optimizer.zero_grad()            # clear gradients of all optimized torch.Tensors'\n",
    "        outputs = model(inputs)          # forward propagation return the value of softmax function\n",
    "        loss = lossFunction(outputs, targets) #compute loss\n",
    "        loss.backward()                  # compute gradient of loss over parameters \n",
    "        optimizer.step()                 # update parameters with gradient descent \n",
    "\n",
    "        train_loss += loss.item()        # accumulate every batch loss in a epoch\n",
    "        _, predicted = outputs.max(1)    # make prediction according to the outputs\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item() # count how many predictions is correct\n",
    "        \n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            # print loss and acc\n",
    "            print( 'Train loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    print( 'Train loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    \n",
    "    return (train_loss * 1.0 / (len(trainloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. 测试（评估）函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, lossFunction, optimizer, device, testloader):\n",
    "    \"\"\"\n",
    "    test model's prediction performance on loader.  \n",
    "    When thid function is called, model is evaluated.\n",
    "    Args:\n",
    "        loader: data for evaluation\n",
    "        model: prediction model\n",
    "        loss_fn: loss function to judge the distance between target and outputs\n",
    "    output:\n",
    "        total_loss\n",
    "        accuracy\n",
    "    \"\"\"\n",
    "    global best_acc\n",
    "    model.eval() #enter test mode\n",
    "    test_loss = 0 # accumulate every batch loss in a epoch\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = lossFunction(outputs, targets) #compute loss\n",
    "\n",
    "            test_loss += loss.item() # accumulate every batch loss in a epoch\n",
    "            _, predicted = outputs.max(1) # make prediction according to the outputs\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item() # count how many predictions is correct\n",
    "        # print loss and acc\n",
    "        print('Test Loss: %.3f  | Test Acc: %.3f%% (%d/%d)'\n",
    "            % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "        accuracy = correct*100.0 / total\n",
    "        \n",
    "    return accuracy, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. 拟合函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, num_epochs, optim_args, trainloader, val_loader, device):\n",
    "    model.to(device)\n",
    "    if device == 'cuda:3':\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    # define the loss function and optimizer\n",
    "\n",
    "    lossFunction = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), \n",
    "                          lr=optim_args[0], \n",
    "                          momentum=optim_args[1], \n",
    "                          weight_decay=optim_args[2])\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accs = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        tr_loss = train(epoch, model, lossFunction, optimizer, device, trainloader)\n",
    "        \n",
    "        message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch+1, \\\n",
    "                                                                num_epochs, loss)\n",
    "        print(message)\n",
    "        \n",
    "        val_accuracy, val_loss = eval(model, lossFunction, optimizer, device, val_loader)\n",
    "        \n",
    "        message = 'Epoch: {}/{}. Validation set: Average loss: {:.4f}, Accuracy: {:.4f}%'.format(epoch+1, \\\n",
    "                                                                num_epochs, val_loss, val_accuracy)\n",
    "        print(message)\n",
    "        \n",
    "        train_losses.append(loss)\n",
    "        val_accs.append(val_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0 :\n",
    "            lr = lr / 10\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        \n",
    "    # show curve\n",
    "    show_curve(train_losses, \"train loss\")\n",
    "    show_curve(val_losses, \"validation loss\")\n",
    "    show_curve(val_accs, \"validation accuracy\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. 初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "\n",
    "optim_args = []\n",
    "optim_args.append(lr)\n",
    "optim_args.append(momentum)\n",
    "optim_args.append(weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model into GPU device\n",
    "device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "args = [2, 5, 4, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Desenet_model = densenet(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(Densenet_model, num_epochs, optim_args, train_loader, valid_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show parameters in model\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in Desenet_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", Desenet_model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"\\nOptimizer's state_dict:\")\n",
    "for var_name in optimzer.state_dict():\n",
    "    print(var_name, \"\\t\", optimzer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "save_path = './model/densenet_model.pt'\n",
    "torch.save(vggnet.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 加载test集数据，使用已训练的模型预测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. 通过路径加载图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testImageLoader(image_name, test_trans):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "    image = Image.open(image_name).convert('RGB')\n",
    "    image = test_trans(image)\n",
    "    image = image.unsqueeze(0)     \n",
    "    return image  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. 预测测试集结果形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "sample_submission.columns = ['file', 'species']\n",
    "# sample_submission['category_id'] = 0\n",
    "sample_submission.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. 预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, transform, sample_submission, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    columns = ['file', 'species']\n",
    "    df_pred = pd.DataFrame(data=np.zeros((0, len(columns))), columns=columns)\n",
    "    \n",
    "    test_dir = './data/test'\n",
    "    \n",
    "    for index, row in (sample_submission.iterrows()):\n",
    "        currImage = os.path.join(test_dir, row['file'])\n",
    "        \n",
    "        if os.path.isfile(currImage):\n",
    "            text_img = testImageLoader (currImage, transform)\n",
    "            \n",
    "            text_img = text_img.to(device)\n",
    "            \n",
    "            # get the index of the max probability\n",
    "            pro = (model(text_img)).data.max(1)[1]\n",
    "            \n",
    "            # get the index of classes for the predicting img\n",
    "            img_index = (pro.cpu().numpy().item())\n",
    "            \n",
    "            # add into data frame\n",
    "            df_pred = df_pred.append({'file': row['file'], 'species': num_to_class[int(img_index)]}, ignore_index=True) \n",
    "        \n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4. 预测结果并写入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = predict(new_vgg, valid_trans, sample_submission, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.to_csv('./result/densenet_result.csv', columns=('file', 'species'), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 使用训练过的模型来初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './model/densenet_model.pt'\n",
    "saved_parametes = torch.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initailze model by saved parameters\n",
    "new_model = VGG(cfg['VGG16'])\n",
    "new_model.load_state_dict(saved_parametes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
